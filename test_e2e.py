#!/usr/bin/env python3
"""
End-to-End (E2E) Test

Enterprise-grade end-to-end testing for the OrcAgent project.
Tests the complete system workflow by invoking main.py and asserting on log files.
"""

import unittest
import os
import sys
import tempfile
import shutil
import time
import subprocess
import json
import re
from pathlib import Path
from typing import Optional, List, Dict, Any

# Add project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


class TestE2E(unittest.TestCase):
    """
    End-to-End tests that validate the complete system workflow.
    
    These tests:
    1. Use main.py as the entry point for all operations
    2. Assert on log files generated by main.py
    3. Verify agent interactions with AWS infrastructure
    4. Test the complete workflow from prompt to deployment
    """

    @classmethod
    def setUpClass(cls):
        """Set up E2E test class."""
        print("üöÄ Starting End-to-End Test Suite")
        print("=" * 80)
        
        # Verify required environment variables
        cls.test_aws_region = os.getenv('AWS_DEFAULT_REGION')
        
        if not cls.test_aws_region:
            raise ValueError(
                "E2E tests require AWS_DEFAULT_REGION environment variable"
            )
        
        # Verify OpenAI API key is available
        cls.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not cls.openai_api_key:
            raise ValueError(
                "E2E tests require OPENAI_API_KEY environment variable"
            )

        # Verify GitHub token is available
        cls.github_token = os.getenv('GITHUB_TOKEN')
        if not cls.github_token:
            raise ValueError(
                "E2E tests require GITHUB_TOKEN environment variable"
            )

        # Verify Notion API key is available
        cls.notion_api_key = os.getenv('NOTION_API_KEY')
        if not cls.notion_api_key:
            raise ValueError(
                "E2E tests require NOTION_API_KEY environment variable"
            )
        
        print(f"üîß E2E Test Environment Configuration:")
        print(f"   üìç AWS Region: {cls.test_aws_region}")
        print(f"   ü§ñ OpenAI API Key: {'***' + cls.openai_api_key[-4:] if cls.openai_api_key else 'NOT SET'}")
        print(f"   üêô GitHub Token: {'***' + cls.github_token[-4:] if cls.github_token else 'NOT SET'}")
        print(f"   üìù Notion API Key: {'***' + cls.notion_api_key[-4:] if cls.notion_api_key else 'NOT SET'}")
        print(f"   üèóÔ∏è Using test AWS account/environment")

    def setUp(self):
        """Set up individual test fixtures."""
        # Create temporary directory for test artifacts
        self.temp_dir = tempfile.mkdtemp()
        
        print(f"\nüß™ E2E Test Setup Complete")
        print(f"   üìÅ Temp directory: {self.temp_dir}")

    def tearDown(self):
        """Clean up test fixtures."""
        # Clean up temporary directory
        shutil.rmtree(self.temp_dir, ignore_errors=True)
        
    def _execute_main_py(self, prompt: str, additional_args: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Execute main.py with given prompt and return results.
        
        Args:
            prompt: Prompt to send to main.py
            additional_args: Additional command line arguments
            
        Returns:
            Dict containing execution results, log path, and status
        """
        # Prepare command
        cmd = [sys.executable, "main.py", "--prompt", prompt, "--clean"]
        
        if additional_args:
            cmd.extend(additional_args)
        
        print(f"   üéØ Executing: {' '.join(cmd)}")
        
        # Execute main.py
        try:
            result = subprocess.run(
                cmd,
                cwd=str(project_root),
                capture_output=True,
                text=True,
                timeout=1200  # 20 minute timeout for complex operations
            )
            
            # Extract log file path from output
            log_file_path = None
            for line in result.stdout.split('\n'):
                if "Results logged to:" in line:
                    log_file_path = line.split("Results logged to: ")[-1].strip()
                    break
            
            return {
                'success': result.returncode == 0,
                'returncode': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'log_file_path': log_file_path,
                'execution_time': time.time()
            }
            
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'returncode': -1,
                'stdout': '',
                'stderr': 'Process timed out after 20 minutes',
                'log_file_path': None,
                'execution_time': time.time()
            }
        except Exception as e:
            return {
                'success': False,
                'returncode': -1,
                'stdout': '',
                'stderr': str(e),
                'log_file_path': None,
                'execution_time': time.time()
            }

    def _analyze_log_file(self, log_file_path: str) -> Dict[str, Any]:
        """
        Analyze the log file generated by main.py.
        
        Args:
            log_file_path: Path to the log file
            
        Returns:
            Dict containing log analysis results
        """
        if not log_file_path or not os.path.exists(log_file_path):
            return {
                'exists': False,
                'content': '',
                'lines': [],
                'environment_mentions': 0,
                'aws_operations': 0,
                'docker_operations': 0,
                'errors': 0,
                'successes': 0
            }
        
        try:
            with open(log_file_path, 'r') as f:
                content = f.read()
            
            lines = content.split('\n')
            
            # Analyze log content
            environment_mentions = content.count('test environment') + content.count('test-')
            aws_operations = content.count('AWS') + content.count('Fargate') + content.count('ECR')
            docker_operations = content.count('Docker') + content.count('docker')
            errors = content.count('‚ùå') + content.count('ERROR')
            successes = content.count('‚úÖ') + content.count('SUCCESS')
            
            return {
                'exists': True,
                'content': content,
                'lines': lines,
                'line_count': len(lines),
                'environment_mentions': environment_mentions,
                'aws_operations': aws_operations,
                'docker_operations': docker_operations,
                'errors': errors,
                'successes': successes,
                'file_size': len(content)
            }
            
        except Exception as e:
            return {
                'exists': False,
                'content': '',
                'lines': [],
                'error': str(e),
                'environment_mentions': 0,
                'aws_operations': 0,
                'docker_operations': 0,
                'errors': 0,
                'successes': 0
            }

    def _extract_json_from_result(self, result: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Extract JSON from main.py execution result.
        
        Args:
            result: Execution result from _execute_main_py
            
        Returns:
            Parsed JSON object or None if not found
        """
        combined_output = result.get('stdout', '') + result.get('stderr', '')
        
        # Check log file content as well
        if result.get('log_file_path'):
            log_analysis = self._analyze_log_file(result['log_file_path'])
            if log_analysis['exists']:
                combined_output += log_analysis['content']
        
        # Look for JSON blocks in markdown format
        json_pattern = r'```json\s*(\{[^`]*\})\s*```'
        json_matches = re.findall(json_pattern, combined_output, re.DOTALL)
        
        if json_matches:
            try:
                # Parse the last JSON found
                json_str = json_matches[-1].strip()
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                print(f"   ‚ö†Ô∏è JSON parsing error: {e}")
                return None
        
        # Look for standalone JSON objects
        json_pattern = r'\{[^{}]*"[^"]*":\s*[^{}]*\}'
        json_matches = re.findall(json_pattern, combined_output)
        
        if json_matches:
            try:
                # Parse the last JSON found
                json_str = json_matches[-1].strip()
                return json.loads(json_str)
            except json.JSONDecodeError:
                pass
        
        return None

    def _extract_playwright_text_from_result(self, result: Dict[str, Any]) -> Optional[str]:
        """
        Extract Playwright visible text dump from main.py execution result.
        
        Args:
            result: Execution result from _execute_main_py
            
        Returns:
            Playwright text dump or None if not found
        """
        combined_output = result.get('stdout', '') + result.get('stderr', '')
        
        # Check log file content as well
        if result.get('log_file_path'):
            log_analysis = self._analyze_log_file(result['log_file_path'])
            if log_analysis['exists']:
                combined_output += log_analysis['content']
        
        # Look for Playwright text dump patterns
        playwright_patterns = [
            r'Playwright visible text:\s*(.+?)(?=\n\n|\nTest|\n‚úÖ|\n‚ùå|$)',
            r'Page text content:\s*(.+?)(?=\n\n|\nTest|\n‚úÖ|\n‚ùå|$)',
            r'UI verification:\s*(.+?)(?=\n\n|\nTest|\n‚úÖ|\n‚ùå|$)'
        ]
        
        for pattern in playwright_patterns:
            matches = re.findall(pattern, combined_output, re.DOTALL)
            if matches:
                return matches[-1].strip()
        
        return None

    def test_notion_workflow(self):
        """
        Test basic agent workflow through main.py.
        Verifies that agents can be initialized and execute simple tasks.
        """
        print("\nü§ñ Test: Notion Workflow")
        
        prompt = """
        Create a notion space with pages for a product plan, design, and requirements 
        for a website of new concept of an inner city coworking space for bouldering hobbyists.
        Then explicitly read all the notion content and write it to a file as markdown.
        Finally respond with the number of notion pages and file line count as a json object, e.g.
        ```json
        {
            "notion_pages": 3,
            "file_line_count": 100
        }
        ```
        """
        
        # Execute main.py
        result = self._execute_main_py(prompt)
        
        # Verify execution succeeded
        if not result['success']:
            print(f"   ‚ùå Execution failed: {result['stderr']}")
            self.fail(f"main.py execution failed: {result['stderr']}")
        
        print(f"   ‚úÖ main.py executed successfully")
        print(f"   üìã Return code: {result['returncode']}")
        
        # Verify log file was created
        self.assertIsNotNone(result['log_file_path'], "Log file path should be returned")
        
        # Analyze log file
        log_analysis = self._analyze_log_file(result['log_file_path'])
        
        # Verify log file exists and contains expected content
        self.assertTrue(log_analysis['exists'], "Log file should exist")
        self.assertGreater(log_analysis['line_count'], 10, "Log file should contain substantial content")
        
        # Verify environment configuration is mentioned
        self.assertGreater(log_analysis['environment_mentions'], 0, 
                          "Log should mention test environment")
        
        # Verify workflow execution indicators
        self.assertGreater(log_analysis['successes'], 0, 
                          "Log should contain success indicators")
        
        # Extract and verify JSON response
        json_result = self._extract_json_from_result(result)
        self.assertIsNotNone(json_result, "JSON response should be present in output")
        
        # Verify JSON contains expected keys
        self.assertIn('notion_pages', json_result, "JSON should contain 'notion_pages' key")
        self.assertIn('file_line_count', json_result, "JSON should contain 'file_line_count' key")
        
        # Verify JSON values are reasonable
        self.assertGreater(json_result['notion_pages'], 0, "Should have created at least one notion page")
        self.assertGreater(json_result['file_line_count'], 0, "Should have generated content in file")
        
        print(f"   ‚úÖ Log analysis successful:")
        print(f"     üìä Lines: {log_analysis['line_count']}")
        print(f"     üèóÔ∏è Environment mentions: {log_analysis['environment_mentions']}")
        print(f"     ‚úÖ Successes: {log_analysis['successes']}")
        print(f"     ‚ùå Errors: {log_analysis['errors']}")
        print(f"   ‚úÖ JSON validation successful:")
        print(f"     üìù Notion pages: {json_result['notion_pages']}")
        print(f"     üìÑ File line count: {json_result['file_line_count']}")

    def test_docker_dev_deployment(self):
        """
        Test Docker development deployment workflow.
        Creates a dockerfile for HTML hosting, pushes to dev, verifies with Playwright.
        """
        print("\nüê≥ Test: Docker Dev Deployment")
        
        prompt = """
        Create a dockerfile for hosting HTML with the text "Hello dev world".
        Push this application to the dev environment on AWS Fargate.
        Read the successful instance logs and include them in your response.
        Verify the deployment using Playwright by accessing the application URL.
        Dump all visible text from the page.
        Finally respond with a json object containing the deployment status and visible text, e.g.
        ```json
        {
            "deployment_status": "success",
            "visible_text": "Hello dev world",
            "instance_logs_found": true
        }
        ```
        """
        
        # Execute main.py
        result = self._execute_main_py(prompt)
        
        # Verify execution succeeded
        if not result['success']:
            print(f"   ‚ùå Execution failed: {result['stderr']}")
            self.fail(f"main.py execution failed: {result['stderr']}")
        
        print(f"   ‚úÖ main.py executed successfully")
        print(f"   üìã Return code: {result['returncode']}")
        
        # Verify log file was created
        self.assertIsNotNone(result['log_file_path'], "Log file path should be returned")
        
        # Analyze log file
        log_analysis = self._analyze_log_file(result['log_file_path'])
        
        # Verify log file exists and contains expected content
        self.assertTrue(log_analysis['exists'], "Log file should exist")
        self.assertGreater(log_analysis['line_count'], 10, "Log file should contain substantial content")
        
        # Verify Docker and AWS operations
        self.assertGreater(log_analysis['docker_operations'], 0, 
                          "Log should contain Docker operations")
        self.assertGreater(log_analysis['aws_operations'], 0, 
                          "Log should contain AWS operations")
        
        # Extract and verify JSON response
        json_result = self._extract_json_from_result(result)
        self.assertIsNotNone(json_result, "JSON response should be present in output")
        
        # Verify JSON contains expected keys
        self.assertIn('deployment_status', json_result, "JSON should contain 'deployment_status' key")
        self.assertIn('visible_text', json_result, "JSON should contain 'visible_text' key")
        self.assertIn('instance_logs_found', json_result, "JSON should contain 'instance_logs_found' key")
        
        # Verify deployment was successful
        self.assertEqual(json_result['deployment_status'], 'success', 
                        "Deployment should be successful")
        
        # Verify visible text contains expected content
        self.assertIn('Hello dev world', json_result['visible_text'], 
                      "Visible text should contain 'Hello dev world'")
        
        # Verify instance logs were found
        self.assertTrue(json_result['instance_logs_found'], 
                       "Instance logs should be found and read")
        
        # Extract Playwright text dump
        playwright_text = self._extract_playwright_text_from_result(result)
        self.assertIsNotNone(playwright_text, "Playwright text dump should be present")
        self.assertIn('Hello dev world', playwright_text, 
                      "Playwright text dump should contain 'Hello dev world'")
        
        print(f"   ‚úÖ Docker deployment analysis successful:")
        print(f"     üìä Lines: {log_analysis['line_count']}")
        print(f"     üê≥ Docker operations: {log_analysis['docker_operations']}")
        print(f"     ‚òÅÔ∏è AWS operations: {log_analysis['aws_operations']}")
        print(f"     ‚úÖ Successes: {log_analysis['successes']}")
        print(f"     ‚ùå Errors: {log_analysis['errors']}")
        print(f"   ‚úÖ JSON validation successful:")
        print(f"     üöÄ Deployment status: {json_result['deployment_status']}")
        print(f"     üìÑ Visible text: {json_result['visible_text']}")
        print(f"     üìã Instance logs found: {json_result['instance_logs_found']}")

    def test_cicd_pipeline_workflow(self):
        """
        Test CI/CD pipeline workflow with GitHub Actions.
        Creates CI/CD pipeline, dockerized HTML app, pushes to PR, merges, verifies with Playwright.
        """
        print("\n‚öôÔ∏è Test: CI/CD Pipeline Workflow")
        
        prompt = """
        Create a CI/CD pipeline for a dockerized HTML application that deploys through test and prod environments.
        Create the dockerized HTML app with the text "Hello prod world".
        Push this to a pull request on GitHub.
        Merge the PR to trigger the CI/CD pipeline.
        Verify the GitHub Action completed successfully.
        Read the successful instance logs of both test and prod environments.
        Verify both test and prod deployments using Playwright by accessing the application URLs.
        Dump all visible text from both pages.
        Finally respond with a json object containing the pipeline status and visible text, e.g.
        ```json
        {
            "pipeline_status": "success",
            "github_action_status": "completed",
            "test_env_text": "Hello prod world",
            "prod_env_text": "Hello prod world",
            "test_logs_found": true,
            "prod_logs_found": true
        }
        ```
        """
        
        # Execute main.py
        result = self._execute_main_py(prompt)
        
        # Verify execution succeeded
        if not result['success']:
            print(f"   ‚ùå Execution failed: {result['stderr']}")
            self.fail(f"main.py execution failed: {result['stderr']}")
        
        print(f"   ‚úÖ main.py executed successfully")
        print(f"   üìã Return code: {result['returncode']}")
        
        # Verify log file was created
        self.assertIsNotNone(result['log_file_path'], "Log file path should be returned")
        
        # Analyze log file
        log_analysis = self._analyze_log_file(result['log_file_path'])
        
        # Verify log file exists and contains expected content
        self.assertTrue(log_analysis['exists'], "Log file should exist")
        self.assertGreater(log_analysis['line_count'], 10, "Log file should contain substantial content")
        
        # Verify Docker and AWS operations
        self.assertGreater(log_analysis['docker_operations'], 0, 
                          "Log should contain Docker operations")
        self.assertGreater(log_analysis['aws_operations'], 0, 
                          "Log should contain AWS operations")
        
        # Extract and verify JSON response
        json_result = self._extract_json_from_result(result)
        self.assertIsNotNone(json_result, "JSON response should be present in output")
        
        # Verify JSON contains expected keys
        required_keys = [
            'pipeline_status', 'github_action_status', 'test_env_text', 
            'prod_env_text', 'test_logs_found', 'prod_logs_found'
        ]
        for key in required_keys:
            self.assertIn(key, json_result, f"JSON should contain '{key}' key")
        
        # Verify pipeline was successful
        self.assertEqual(json_result['pipeline_status'], 'success', 
                        "Pipeline should be successful")
        
        # Verify GitHub Action completed
        self.assertEqual(json_result['github_action_status'], 'completed', 
                        "GitHub Action should be completed")
        
        # Verify visible text contains expected content for both environments
        self.assertIn('Hello prod world', json_result['test_env_text'], 
                      "Test environment text should contain 'Hello prod world'")
        self.assertIn('Hello prod world', json_result['prod_env_text'], 
                      "Prod environment text should contain 'Hello prod world'")
        
        # Verify logs were found for both environments
        self.assertTrue(json_result['test_logs_found'], 
                       "Test environment logs should be found")
        self.assertTrue(json_result['prod_logs_found'], 
                       "Prod environment logs should be found")
        
        # Extract Playwright text dump
        playwright_text = self._extract_playwright_text_from_result(result)
        self.assertIsNotNone(playwright_text, "Playwright text dump should be present")
        self.assertIn('Hello prod world', playwright_text, 
                      "Playwright text dump should contain 'Hello prod world'")
        
        print(f"   ‚úÖ CI/CD pipeline analysis successful:")
        print(f"     üìä Lines: {log_analysis['line_count']}")
        print(f"     üê≥ Docker operations: {log_analysis['docker_operations']}")
        print(f"     ‚òÅÔ∏è AWS operations: {log_analysis['aws_operations']}")
        print(f"     ‚úÖ Successes: {log_analysis['successes']}")
        print(f"     ‚ùå Errors: {log_analysis['errors']}")
        print(f"   ‚úÖ JSON validation successful:")
        print(f"     ‚öôÔ∏è Pipeline status: {json_result['pipeline_status']}")
        print(f"     üêô GitHub Action status: {json_result['github_action_status']}")
        print(f"     üß™ Test env text: {json_result['test_env_text']}")
        print(f"     üöÄ Prod env text: {json_result['prod_env_text']}")
        print(f"     üìã Test logs found: {json_result['test_logs_found']}")
        print(f"     üìã Prod logs found: {json_result['prod_logs_found']}")

    def test_solo_founder_workflow(self):
        """
        E2E test for solo founder mode using main.py --agents=solo
        """
        prompt = "Perform a solo founder dry run."
        result = self._execute_main_py(prompt, additional_args=["--agents=solo"])
        self.assertIn("log_file", result)
        log_file_path = result["log_file"]
        self.assertTrue(os.path.exists(log_file_path))
        with open(log_file_path, "r") as f:
            log_content = f.read()
        self.assertIn("Successfully created solo founder group chat", log_content)
        self.assertIn("WORKFLOW EXECUTION STARTED", log_content)
        self.assertIn("WORKFLOW EXECUTION COMPLETED", log_content)


if __name__ == '__main__':
    # Run tests with high verbosity
    unittest.main(verbosity=2)
